{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 â€“ Working with Text Data & Embeddings\n",
    "## Based on *Build a Large Language Model (From Scratch)* by Sebastian Raschka\n",
    "\n",
    "This notebook walks through the core ideas in Chapter 2: tokenization, vocabulary building, the sliding-window dataset, and token embeddings. Each major section is preceded by a personal explanation of **why** the step matters â€” not just for understanding LLMs, but also for building robust agentic systems that rely on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“Œ Personal Explanation 1 â€” Why Tokenization is the Foundation\n",
    "\n",
    "Neural networks operate on numbers, not words. Tokenization is the bridge that converts raw text into a sequence of integer IDs that the model can process. The choice of tokenization strategy has cascading effects on everything downstream:\n",
    "\n",
    "- **Vocabulary size** directly controls the size of the embedding matrix and the output projection layer. Larger vocabularies mean more parameters but better coverage of rare words.\n",
    "- **Sub-word tokenization** (BPE, used by GPT) balances coverage and compactness: common words get a single token, rare words are split into meaningful sub-units. This prevents the model from seeing entirely `<UNK>` tokens for anything outside the training distribution.\n",
    "- **For agentic systems**, tokenization determines *how* tool names, JSON keys, code identifiers, and natural-language instructions are chunked. Poor tokenization of structured data (e.g., splitting a UUID mid-token) can confuse the model and cause subtle reasoning errors.\n",
    "\n",
    "In short: garbage tokenization â†’ garbage inputs â†’ unreliable outputs, no matter how powerful the model architecture is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d50359d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: torch in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2026.2.19)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\david\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2026.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\david\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken available : True\n",
      "torch    available : True\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Standard imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import re\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# â”€â”€ Optional heavy dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# If tiktoken / torch are available we use them; otherwise we fall back to our\n",
    "# hand-rolled implementations so the notebook can still be read end-to-end.\n",
    "HAS_TIKTOKEN = importlib.util.find_spec('tiktoken') is not None\n",
    "HAS_TORCH    = importlib.util.find_spec('torch')    is not None\n",
    "\n",
    "print(f'tiktoken available : {HAS_TIKTOKEN}')\n",
    "print(f'torch    available : {HAS_TORCH}')\n",
    "\n",
    "if HAS_TIKTOKEN:\n",
    "    import tiktoken\n",
    "if HAS_TORCH:\n",
    "    import torch\n",
    "    import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.1 â€” Loading the raw text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in corpus : 20,479\n",
      "--- First 200 characters ---\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a\n"
     ]
    }
   ],
   "source": [
    "# Adjust the path to wherever you placed the file\n",
    "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f'Total characters in corpus : {len(raw_text):,}')\n",
    "print('--- First 200 characters ---')\n",
    "print(raw_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2 â€” Simple regex-based tokenizer (from the book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens : 4,690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# Split on whitespace AND common punctuation so both become separate tokens.\n",
    "# The capturing group keeps the delimiters themselves.\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "print(f'Number of tokens : {len(preprocessed):,}')\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size : 1132\n",
      "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '--': 6, '.': 7, ':': 8, ';': 9}\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary: sorted unique tokens + two special tokens\n",
    "all_tokens = sorted(set(preprocessed))\n",
    "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
    "vocab_size = len(all_tokens)\n",
    "print(f'Vocabulary size : {vocab_size}')\n",
    "\n",
    "# String â†’ int and int â†’ string mappings\n",
    "str_to_int = {tok: idx for idx, tok in enumerate(all_tokens)}\n",
    "int_to_str = {idx: tok for tok, idx in str_to_int.items()}\n",
    "\n",
    "# Peek at a slice of the vocab\n",
    "print(dict(list(str_to_int.items())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "Decoded: <|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    \"\"\"Character-punctuation tokenizer with <|unk|> for OOV words.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab: dict):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        tokens = [t.strip() for t in tokens if t.strip()]\n",
    "        tokens = [t if t in self.str_to_int else '<|unk|>' for t in tokens]\n",
    "        return [self.str_to_int[t] for t in tokens]\n",
    "\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        text = ' '.join(self.int_to_str[i] for i in ids)\n",
    "        # Remove spaces before punctuation (cosmetic)\n",
    "        return re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(str_to_int)\n",
    "\n",
    "sample = 'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'\n",
    "ids = tokenizer.encode(sample)\n",
    "print('Encoded:', ids)\n",
    "print('Decoded:', tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.3 â€” Byte-Pair Encoding (BPE) with `tiktoken`\n",
    "\n",
    "The simple tokenizer above is instructive but fragile â€” it can't handle any word that wasn't in the training text. GPT-2/3/4 use **Byte-Pair Encoding (BPE)**, which builds a vocabulary of sub-word units through iterative merging of the most frequent pairs of bytes. The result: every possible string can be encoded (worst-case as individual bytes), and the vocabulary is compact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE token count  : 5,145\n",
      "BPE vocab size   : 50,257\n",
      "Round-trip encodeâ†’decode : âœ“\n"
     ]
    }
   ],
   "source": [
    "if HAS_TIKTOKEN:\n",
    "    tokenizer_bpe = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "    integers = tokenizer_bpe.encode(raw_text, allowed_special={'<|endoftext|>'})\n",
    "    print(f'BPE token count  : {len(integers):,}')\n",
    "    print(f'BPE vocab size   : {tokenizer_bpe.n_vocab:,}')\n",
    "\n",
    "    # Round-trip test\n",
    "    decoded = tokenizer_bpe.decode(integers)\n",
    "    assert decoded == raw_text, 'Round-trip failed!'\n",
    "    print('Round-trip encodeâ†’decode : âœ“')\n",
    "else:\n",
    "    print('tiktoken not installed â€” skipping BPE section.')\n",
    "    print('Install with:  pip install tiktoken')\n",
    "    # Provide integer list from simple tokenizer so rest of notebook still runs\n",
    "    integers = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“Œ Personal Explanation 2 â€” The Sliding-Window Dataset\n",
    "\n",
    "Language models are trained to predict the **next token** given a context window of preceding tokens. The sliding-window (or *stride*) approach is how we manufacture (input, target) pairs from a single long document:\n",
    "\n",
    "```\n",
    "tokens : [t0 t1 t2 t3 t4 t5 t6 t7 ...]\n",
    "\n",
    "window 1 â†’  input=[t0..t3]   target=[t1..t4]   (stride=1)\n",
    "window 2 â†’  input=[t1..t4]   target=[t2..t5]\n",
    "...\n",
    "```\n",
    "\n",
    "**Why overlap (stride < max_length) is useful:**  \n",
    "When stride equals `max_length`, each token appears in exactly one training example as context. With a smaller stride, the model sees each token in *multiple different contexts*, which acts like implicit data augmentation. The model learns that the same word can play different semantic roles depending on what precedes it â€” crucial for learning long-range dependencies.\n",
    "\n",
    "**For agentic systems**, context windows behave like a special case of this sliding window: only the last `N` tokens of an agent's scratchpad are visible at inference time. Understanding stride helps reason about *information retention* across long agent trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens available : 4,690\n",
      "  max_length=4, stride=1  â†’  4,686 samples\n",
      "  max_length=4, stride=2  â†’  2,343 samples\n",
      "  max_length=4, stride=4  â†’  1,172 samples\n",
      "\n",
      "First sample:\n",
      "  input  : ['I', 'HAD', 'always', 'thought']\n",
      "  target : ['HAD', 'always', 'thought', 'Jack']\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Sliding-window data generation (pure Python, no torch needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def create_dataloader_samples(token_ids: list[int],\n",
    "                               max_length: int,\n",
    "                               stride: int) -> tuple[list, list]:\n",
    "    \"\"\"Returns (input_chunks, target_chunks) lists of token-id lists.\"\"\"\n",
    "    inputs, targets = [], []\n",
    "    for i in range(0, len(token_ids) - max_length, stride):\n",
    "        inputs.append(token_ids[i : i + max_length])\n",
    "        targets.append(token_ids[i + 1 : i + max_length + 1])\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "# Quick test with the simple tokenizer ids\n",
    "sample_ids = tokenizer.encode(raw_text)\n",
    "print(f'Total tokens available : {len(sample_ids):,}')\n",
    "\n",
    "for ml, st in [(4, 1), (4, 2), (4, 4)]:\n",
    "    inp, tgt = create_dataloader_samples(sample_ids, max_length=ml, stride=st)\n",
    "    print(f'  max_length={ml}, stride={st}  â†’  {len(inp):,} samples')\n",
    "\n",
    "# Show a concrete example\n",
    "inp, tgt = create_dataloader_samples(sample_ids, max_length=4, stride=1)\n",
    "print('\\nFirst sample:')\n",
    "print('  input  :', [int_to_str[i] for i in inp[0]])\n",
    "print('  target :', [int_to_str[i] for i in tgt[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ§ª Experiment â€” Effect of `max_length` and `stride` on Sample Count\n",
    "\n",
    "Let's systematically vary both parameters and record the number of samples produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config                                    Samples  Formula: (N - max_length) // stride\n",
      "------------------------------------------------------------\n",
      "No overlap (stride = window)                  146  (expected â‰ˆ 145)\n",
      "50% overlap                                   292  (expected â‰ˆ 291)\n",
      "75% overlap                                   583  (expected â‰ˆ 582)\n",
      "Maximum overlap (stride = 1)                 4658  (expected â‰ˆ 4658)\n",
      "Larger window, no overlap                      73  (expected â‰ˆ 72)\n",
      "Larger window, 50% overlap                    145  (expected â‰ˆ 144)\n",
      "Even larger window, 50% overlap                72  (expected â‰ˆ 71)\n",
      "\n",
      "Total tokens in corpus: 4,690\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Experiment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "configs = [\n",
    "    # (max_length, stride, description)\n",
    "    (32,  32, 'No overlap (stride = window)'),\n",
    "    (32,  16, '50% overlap'),\n",
    "    (32,   8, '75% overlap'),\n",
    "    (32,   1, 'Maximum overlap (stride = 1)'),\n",
    "    (64,  64, 'Larger window, no overlap'),\n",
    "    (64,  32, 'Larger window, 50% overlap'),\n",
    "    (128, 64, 'Even larger window, 50% overlap'),\n",
    "]\n",
    "\n",
    "print(f\"{'Config':<40} {'Samples':>8}  Formula: (N - max_length) // stride\")\n",
    "print('-' * 60)\n",
    "N = len(sample_ids)\n",
    "for ml, st, desc in configs:\n",
    "    inp, _ = create_dataloader_samples(sample_ids, max_length=ml, stride=st)\n",
    "    expected = (N - ml) // st   # analytical formula\n",
    "    print(f'{desc:<40} {len(inp):>8}  (expected â‰ˆ {expected})')\n",
    "\n",
    "print(f'\\nTotal tokens in corpus: {N:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Findings\n",
    "\n",
    "The number of samples follows a simple formula:\n",
    "\n",
    "$$\\text{samples} = \\left\\lfloor \\frac{N - \\text{max\\_length}}{\\text{stride}} \\right\\rfloor$$\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. **Smaller stride â†’ exponentially more training samples** from the same corpus. With `stride=1` on a 4,690-token corpus and `max_length=32`, we get ~4,658 samples vs. ~146 with `stride=32`. That's a **31Ã— increase** just from overlapping windows.\n",
    "\n",
    "2. **Larger `max_length` â†’ richer context per sample, but fewer samples** (for the same stride). There is a genuine trade-off between the richness of context each sample provides and dataset size.\n",
    "\n",
    "3. **Overlap is useful because** it lets the model see each token in many surrounding contexts. The word *painting* appears differently after *\"he stopped\"* versus *\"she loved\"*. With stride=1, the model trains on both. With no overlap, it may only see one.\n",
    "\n",
    "4. For small corpora (like this ~20 KB story), aggressive overlap can be necessary to have enough samples to train even a small model without overfitting. For massive internet-scale datasets, overlap matters less because data volume is not the bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“Œ Personal Explanation 3 â€” Token Embeddings vs. Position Embeddings\n",
    "\n",
    "Two separate embedding tables are summed to produce the input to a transformer:\n",
    "\n",
    "| Embedding type | What it encodes | Size |\n",
    "|---|---|---|\n",
    "| **Token embedding** | Semantic identity of the token | `vocab_size Ã— d_model` |\n",
    "| **Position embedding** | Position of the token in the sequence | `context_length Ã— d_model` |\n",
    "\n",
    "**Why do we need positional embeddings?**  \n",
    "Self-attention is *permutation-invariant* â€” the attention score between token A and token B is the same whether A appears before or after B. Without positional information the model literally cannot tell the difference between *\"the dog bit the man\"* and *\"the man bit the dog\"*. Adding a learned (or sinusoidal) position vector breaks this symmetry.\n",
    "\n",
    "**For agentic systems** this matters in multi-turn memory: the position of a tool result earlier in the context affects how the model weights it relative to a more recent observation. Absolute position embeddings can degrade at sequence lengths beyond what was seen during training â€” which is why modern architectures use **RoPE** (Rotary Position Embeddings) that generalize better to longer contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids_batch shape : torch.Size([8, 4])\n",
      "tok_emb shape         : torch.Size([8, 4, 256])\n",
      "pos_emb shape         : torch.Size([1, 4, 256])\n",
      "x (input to model)    : torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Token + Position Embedding (PyTorch version) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if HAS_TORCH:\n",
    "    # Hyper-parameters (GPT-2 small scale)\n",
    "    vocab_size_emb = 50257   # GPT-2 BPE vocab\n",
    "    output_dim     = 256     # embedding dimension (d_model)\n",
    "    max_len        = 1024    # maximum sequence / context length\n",
    "    batch_size     = 8\n",
    "    seq_len        = 4\n",
    "\n",
    "    token_embedding_layer = nn.Embedding(vocab_size_emb, output_dim)\n",
    "    pos_embedding_layer   = nn.Embedding(max_len,        output_dim)\n",
    "\n",
    "    # Simulate a batch of token id sequences\n",
    "    token_ids_batch = torch.randint(0, vocab_size_emb, (batch_size, seq_len))\n",
    "    positions       = torch.arange(seq_len).unsqueeze(0)  # shape (1, seq_len)\n",
    "\n",
    "    tok_emb = token_embedding_layer(token_ids_batch)  # (B, T, d)\n",
    "    pos_emb = pos_embedding_layer(positions)           # (1, T, d) â†’ broadcasts\n",
    "\n",
    "    x = tok_emb + pos_emb  # final input to the transformer\n",
    "\n",
    "    print(f'token_ids_batch shape : {token_ids_batch.shape}')\n",
    "    print(f'tok_emb shape         : {tok_emb.shape}')\n",
    "    print(f'pos_emb shape         : {pos_emb.shape}')\n",
    "    print(f'x (input to model)    : {x.shape}')\n",
    "else:\n",
    "    print('torch not installed â€” showing conceptual pseudo-code below.')\n",
    "    print(\"\"\"\n",
    "    token_embedding_layer = Embedding(vocab_size=50257, d_model=256)\n",
    "    pos_embedding_layer   = Embedding(max_len=1024,     d_model=256)\n",
    "\n",
    "    # For a batch of shape (B=8, T=4):\n",
    "    tok_emb = token_embedding_layer(token_ids)   # â†’ (8, 4, 256)\n",
    "    pos_emb = pos_embedding_layer(positions)      # â†’ (1, 4, 256)  [broadcast]\n",
    "    x = tok_emb + pos_emb                         # â†’ (8, 4, 256)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“Œ Personal Explanation 4 â€” Why Do Embeddings Encode Meaning?\n",
    "\n",
    "This is the central conceptual question of the chapter.\n",
    "\n",
    "### The Short Answer\n",
    "Embeddings encode meaning **not because we programmed them to**, but because meaning is what the model *needs* to compress in order to predict the next token correctly. During training, the gradient descent process pushes the embedding vectors for *contextually similar* tokens (words that appear in similar positions in similar sentences) to nearby regions of the embedding space.\n",
    "\n",
    "### The Neural Network Connection\n",
    "An `nn.Embedding(vocab_size, d_model)` layer is just a **look-up table** â€” a matrix `W` of shape `(vocab_size, d_model)`. When you embed token `i`, you retrieve row `W[i]`. There is nothing magical about initialization; all rows start as random vectors.\n",
    "\n",
    "What makes them meaningful is the **loss function + backpropagation**:\n",
    "\n",
    "1. The model predicts a distribution over the next token.\n",
    "2. The cross-entropy loss measures how wrong the prediction is.\n",
    "3. Backprop propagates gradients all the way back through the attention layers into `W`.\n",
    "4. Rows corresponding to tokens that frequently co-occur with the same context tokens get nudged in the same direction â€” they converge in embedding space.\n",
    "\n",
    "This is the distributional hypothesis from linguistics (*\"a word is known by the company it keeps\"*) implemented as gradient descent.\n",
    "\n",
    "### Why This Matters for Agentic Systems\n",
    "An agentic LLM must understand user *intent* (a semantic concept), map that intent to tool invocations, and reason about the tool results. All of this relies on the geometric structure of the embedding space. When you do RAG retrieval, you are literally doing nearest-neighbor search in this space â€” semantic proximity is geometric proximity. The richer and more nuanced the embedding space, the better the agent reasons and retrieves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair                 Cosine Similarity\n",
      "--------------------------------------\n",
      "  king â†” queen          0.816\n",
      "  man â†” woman          0.724\n",
      "  king â†” apple          0.069\n",
      "  apple â†” fruit          0.977\n",
      "  king â†” man            0.979\n",
      "\n",
      "Analogy: king âˆ’ man + woman â†’ ?\n",
      "  sim(king  ) = 0.879\n",
      "  sim(queen ) = 0.993\n",
      "  sim(man   ) = 0.828\n",
      "  sim(woman ) = 0.964\n",
      "  sim(apple ) = -0.042\n",
      "  sim(fruit ) = -0.132\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ Illustrate embedding similarity (no torch needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import math\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def cosine_similarity(a: list[float], b: list[float]) -> float:\n",
    "    dot = sum(x * y for x, y in zip(a, b))\n",
    "    mag_a = math.sqrt(sum(x**2 for x in a))\n",
    "    mag_b = math.sqrt(sum(x**2 for x in b))\n",
    "    return dot / (mag_a * mag_b + 1e-8)\n",
    "\n",
    "# Suppose after training, the model has learned these 4-D embeddings:\n",
    "# (in reality d_model=768 or 1024, but 4D is enough to illustrate)\n",
    "embeddings = {\n",
    "    'king'  : [ 0.9,  0.1,  0.8,  0.3],\n",
    "    'queen' : [ 0.8,  0.9,  0.7,  0.2],  # similar to king\n",
    "    'man'   : [ 0.5,  0.0,  0.6,  0.1],\n",
    "    'woman' : [ 0.4,  0.6,  0.5,  0.0],  # similar to man\n",
    "    'apple' : [-0.2, -0.1,  0.0,  0.9],  # very different\n",
    "    'fruit' : [-0.3, -0.2,  0.1,  0.8],  # similar to apple\n",
    "}\n",
    "\n",
    "pairs = [\n",
    "    ('king', 'queen'),\n",
    "    ('man',  'woman'),\n",
    "    ('king', 'apple'),\n",
    "    ('apple', 'fruit'),\n",
    "    ('king', 'man'),\n",
    "]\n",
    "\n",
    "print(f\"{'Pair':<20} Cosine Similarity\")\n",
    "print('-' * 38)\n",
    "for a, b in pairs:\n",
    "    sim = cosine_similarity(embeddings[a], embeddings[b])\n",
    "    print(f'  {a} â†” {b:<12}   {sim:.3f}')\n",
    "\n",
    "# Classic analogy: king - man + woman â‰ˆ queen\n",
    "v_king  = embeddings['king']\n",
    "v_man   = embeddings['man']\n",
    "v_woman = embeddings['woman']\n",
    "analogy_vec = [k - m + w for k, m, w in zip(v_king, v_man, v_woman)]\n",
    "\n",
    "print('\\nAnalogy: king âˆ’ man + woman â†’ ?')\n",
    "for word, vec in embeddings.items():\n",
    "    sim = cosine_similarity(analogy_vec, vec)\n",
    "    print(f'  sim({word:<6}) = {sim:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.6 â€” Putting It All Together: a `GPTDataset` class\n",
    "\n",
    "The book combines everything into a PyTorch `Dataset` class. We provide both a PyTorch version (if available) and a pure-Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs  shape: torch.Size([8, 4])\n",
      "targets shape: torch.Size([8, 4])\n",
      "inputs : tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "targets: tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n",
      "\n",
      "Final embedding shape (batch, seq, d_model): torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "if HAS_TORCH:\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "    class GPTDatasetV1(Dataset):\n",
    "        def __init__(self, txt: str, tokenizer, max_length: int, stride: int):\n",
    "            self.input_ids  = []\n",
    "            self.target_ids = []\n",
    "\n",
    "            token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "            for i in range(0, len(token_ids) - max_length, stride):\n",
    "                self.input_ids.append(torch.tensor(token_ids[i          : i + max_length]))\n",
    "                self.target_ids.append(torch.tensor(token_ids[i + 1     : i + max_length + 1]))\n",
    "\n",
    "        def __len__(self):          return len(self.input_ids)\n",
    "        def __getitem__(self, idx): return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "    def create_dataloader_v1(txt, tokenizer, batch_size=4, max_length=256,\n",
    "                             stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "        dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "                          drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "\n",
    "    if HAS_TIKTOKEN:\n",
    "        bpe = tiktoken.get_encoding('gpt2')\n",
    "        loader = create_dataloader_v1(raw_text, bpe, batch_size=8,\n",
    "                                      max_length=4, stride=4, shuffle=False)\n",
    "        data_iter = iter(loader)\n",
    "        inputs, targets = next(data_iter)\n",
    "        print('inputs  shape:', inputs.shape)\n",
    "        print('targets shape:', targets.shape)\n",
    "        print('inputs :', inputs)\n",
    "        print('targets:', targets)\n",
    "\n",
    "        # Full embedding pipeline\n",
    "        vocab_size_gpt2 = 50257\n",
    "        output_dim      = 256\n",
    "        context_length  = 4\n",
    "\n",
    "        token_embedding_layer = torch.nn.Embedding(vocab_size_gpt2, output_dim)\n",
    "        pos_embedding_layer   = torch.nn.Embedding(context_length,  output_dim)\n",
    "\n",
    "        tok_emb = token_embedding_layer(inputs)\n",
    "        pos_emb = pos_embedding_layer(torch.arange(context_length))\n",
    "        input_embeddings = tok_emb + pos_emb\n",
    "\n",
    "        print('\\nFinal embedding shape (batch, seq, d_model):', input_embeddings.shape)\n",
    "    else:\n",
    "        print('tiktoken not available â€” skipping GPTDataset demo. Install with: pip install tiktoken')\n",
    "else:\n",
    "    print('torch not available â€” showing pseudo-code only.')\n",
    "    print('Install with: pip install torch')\n",
    "    print()\n",
    "    # Pure-Python fallback to still show the concept\n",
    "    class GPTDatasetV1_Pure:\n",
    "        def __init__(self, token_ids, max_length, stride):\n",
    "            self.input_ids, self.target_ids = [], []\n",
    "            for i in range(0, len(token_ids) - max_length, stride):\n",
    "                self.input_ids.append(token_ids[i : i + max_length])\n",
    "                self.target_ids.append(token_ids[i + 1 : i + max_length + 1])\n",
    "        def __len__(self): return len(self.input_ids)\n",
    "        def __getitem__(self, idx): return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "    ids = tokenizer.encode(raw_text)\n",
    "    ds  = GPTDatasetV1_Pure(ids, max_length=4, stride=2)\n",
    "    print(f'Dataset size: {len(ds)} samples')\n",
    "    inp, tgt = ds[0]\n",
    "    print('input  tokens:', [int_to_str[i] for i in inp])\n",
    "    print('target tokens:', [int_to_str[i] for i in tgt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Step | What we did | Why it matters |\n",
    "|---|---|---|\n",
    "| **Tokenization** | Split text â†’ integer IDs | Neural nets require numeric input; choice of tokenizer affects model capacity and robustness |\n",
    "| **Vocabulary** | Build strâ†”int mappings + special tokens | Defines the input/output space of the model |\n",
    "| **Sliding window** | Create (input, target) pairs with configurable `max_length` / `stride` | Training signal for next-token prediction; overlap = data augmentation |\n",
    "| **Token embedding** | `nn.Embedding` look-up table trained by backprop | Converts discrete tokens to dense vectors; geometric proximity = semantic similarity |\n",
    "| **Position embedding** | Separate learned table summed with token emb | Breaks permutation invariance of attention; lets model track token order |\n",
    "\n",
    "The output of this pipeline â€” a tensor of shape `(batch, seq_len, d_model)` â€” is the input to the transformer's attention layers in Chapter 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
